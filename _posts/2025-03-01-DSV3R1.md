---
layout: post
title: Deepseek V3 & R1 论文解读笔记 技术版
date: 2025-03-01 20:30:13
description: Deep Tech-analysis for DeepSeek New Model V3 & R1
tags: LLM
categories: Tech
featured: true

toc:
  sidebar: left
---

## 前言

大语言模型中 在实用方面 分为两种模型，

一种是 通用模型，一种是 推理模型 推理模型是在通用模型基础上进行 加强（核心技术），

推理模型的核心技术：
强化学习，RL； 推理规模（inference scaling）

### 通用模型

实例：GPT-4

运行原理：预测下一个词（概率预测）

特点：快速

比较依赖提示词补偿，进行显式引导

优势领域：创意

缺陷：幻觉，也就是瞎说

### 推理模型

实例：GPT-o3

运行原理：会对你内容进行思考，也就是CoT

优势：逻辑密度高

优势领域：逻辑推理

缺点：不要使用”启发式“（cosplay）提示词

---

# DeepSeek V3

## 核心特点

`DeepSeek-V3`  

- **架构**：MoE（Mixture of Experts），总参数量 671B，每次激活 37B。
- **关键技术**：
    - **Multi-head Latent Attention (MLA)**：减少推理计算资源。
    - **DeepSeekMoE**：优化训练和推理成本。
    - **FP8 混合精度**：取代 FP32，提升效率。
    - **训练策略**：Multi-token prediction + auxiliary-loss-free。
    - **预训练**：稳定，没有 loss spike & roll back

**MoE 与 Dense 模型区别**

- **MoE**：推理时只激活一部分相关参数。
- **Dense**：推理时激活所有参数。

## Architecture

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/DSV3R1_1.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

### Multi-head Latent Attention(MLA)

<aside>
💫

- 作用：在 inference 的时候 使得计算资源进一步减少
- 基于：Attention & KV cache 上面的改进
</aside>

Attention 本质上是计算 token 之间的 correlation

由于 大部分大语言模型 都采用decoder 架构，所以 autoregressive 所以就是token by token, 相当于 运行decoder N 遍，KV cache 就是一个拿空间换时间的方法。 

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/DSV3R1_2.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

MLA 的基本思想

1. 用一个低维度的 C 去代替 原本的 K & V， 通过 C 去搭配 W_k & W_v 去还原 原本的K & V
2. 但是这个会让 RoPE 失效

Decoupled Rotary Position Embedding (RoPE) - 一种 Position Embedding， 介于 absolute position embedding & relative position embedding 之间，

但这里就会让 KV cache 失效， 所以 在 采用 architecture 中最上面图片 的右下 Attention 结构  

### DeepSeekMoE architectures

<aside>
➡️

是整体架构，主要针对的是training & inference 的 cost 优化，具体改动：

1. 增加 **Expert** 数量，减少单个 Expert 的参数量，从而保持总参数量相近。
2. 将部分 Expert 变为通用 Expert（即通才），提升模型的泛化能力。
3. 在 **sigmoid** 前增加 **bias term**，用于调节负载（仅在训练时使用）。
</aside>

**MoE 架构**

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/DSV3R1_3.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

MoE 架构：

- **Dense MoE**：所有 FFN（Feed-Forward Network）按比例加权平均。
- **Sparse MoE**：选择 Top K 的 Expert 进行加权平均，通常选择 Sparse MoE 以减少成本。

**问题**：在不同的Expert 中间 区分度不是特别大，且有可能会有重合冗余的

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/DSV3R1_4.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

基于上述问题，DeepSeekMoE 架构 

**结构改进**: 

- 专家数量倍增（2N个微专家）
- 设置共享专家（Common Experts）
- 用 数量  交换 质量，Various 变小了，就不会太容易过拟合

### Auxiliary-loss-free

<aside>
💫

**问题**：MoE 模型 在training中非常可能出现负载不平衡 (**Load Balance**) 

**解决方法：**通常 是使用 **Loss Control**, 但作者选择使用 **Loss Free**

</aside>

MoE 模型在训练过程中可能出现 **负载不平衡** 问题，即某些 Expert 被过度使用，导致富的更富，穷的更穷。传统方法通过增加额外的 Loss Control 来平衡负载，但这种**Loss 可能是错误**的！

DeepSeek 采用 **loss free** 的方法

$$
g'_{i,t} =\begin{cases}s_{i,t} + b_i & \text{if } s_{i,t} + b_i \in \text{Top}_k\left( \{ s_{j,t} + b_j \mid 1 \leq j \leq N_r \}, K_r \right), \\0 & \text{otherwise.}\end{cases}
$$

通过在 softmax 前增加 **bias term** 来手动调节负载（token越多，则 b 越小，从而输出的softmax 越小。），从而在不影响损失的情况下实现负载平衡。

### Multi-token prediction

<aside>
➡️

增强模型推理性能 & 增加训练效率

</aside>

**LLM Training**

- **短视**：模型训练时通常会依赖较短期的目标（即单个token的预测），缺乏对长时间步的全局视野。

**LLM Inference**

- **自回归模型**：LLM的推理是**token by token**的逐步生成，每生成一个token就进行一次预测，并以此为基础继续生成下一个token。
- **缺乏Ground Truth**：在推理阶段，模型没有像训练阶段那样有ground truth去进行校正。
- **慢**：推理过程由于是逐步生成，因此速度较慢，尤其是当生成长文本时，推理效率低下。

**思路**：
通过在 Inference 过程中采用 **Speculative Decoding（猜测性解码）**，可以提高生成的速度并减少逐token生成的时间开销。即 增加一个小模型，去快速预测一个token 然后由大模型进行验证

**必要条件**

1. **Quick Guess (快速猜测)**：
    - 需要在推理过程中进行快速的初步猜测，从而减少推理时间。
2. **Cheap Verification (廉价验证)**：
    - 对每个猜测结果进行快速且低成本的验证，确保最终的预测结果准确。

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/DSV3R1_5.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

DeepSeek 采用了 把 Causal Chain 从 推理阶段 放进了 训练阶段，在训练过程中 增强了模型的远视能力，在推理过程中 则 直接拿掉

# Deepseek R1

<aside>

在理解了基础模型与推理模型的区别后，我们将深入解析 R1 这一推理模型。R1 的论文标题为《Incentivizing Reasoning Capability》，其核心思想是通过激励机制增强模型的推理能力。相较于 V3，R1 的设计思路更为直观。

</aside>

## 背景

### 1 RL

**强化学习（RL）** 是一种对泛化能力有显著帮助的学习范式，相当于一种通过激励机制（奖励和惩罚）来优化模型行为的学习范式。模型在回答正确时获得奖励，从而被鼓励自主探索。在 R1 的训练过程中，激励机制主要关注模型是否进行思考以及结果的正确性。

- reward
    - 是 稀疏的
    - 如何去 define 一个好的reward
- dynamic interact with environment
- explore v.s. exploit

### 2 AGI

**通往 AGI 的道路**：推理模型的出现源于对通用人工智能（AGI）的追求。AGI 的核心在于模型的泛化能力，而非简单的死记硬背。因此，模型需要具备自主思考和推理的能力。

### 3 LLM Training

1. 训练范式
    
    原先openai 范式： 
    
    1. Pre-train :  在这个 过程中，数据质量很关键。
    2. Post-train
        1. SFT：这个过程中，数据质量也很关键
        2. human preference → reward model
        3. reward model → RL        (PPO)
    
     R1 的纯RL 是只有rule-based + final reward，也就是像 code & math 
    
- 要训练 LLM 通过 RL 做题，要有 精准的 Reward
- 不要采用结构化的方法，最终限制了模型的效果，要让模型 探索思考方式
- 思考过程中包含了搜索过程，允许犯错
1. scaling law
    
    General = scale up + less structure model
    
    但是 scale 也不能无限大，会遇到各种各样的问题，` 系统问题 `  尤其突出，e.g. loss spike (loss突然增大) ；其次就是越来越容易听不懂人话
    
2. emergent ability 涌现
    
    就是指 大模型参数超过一定的阈值之后，大模型的性能 会突然从量变到质变。
    
    在 R1 这篇文章中也得到了证实。 R1的这套方法在小模型上面没有效果
    

 

## Architecture

### **复现 DeepSeek R1 的「Aha Moment」**

                                                                  ***“左脚踩右脚”***

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/DSV3R1_5.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

1. **基础模型**：`DeepSeek V3`（MoE 模型）。
2. **纯 RL 训练**：通过纯 RL（数学和代码）训练生成 `DS-R1-Zero` 模型。
    - 优点：极强的推理能力。
    - 缺点：可读性差，语言混乱。
    - 生成 **SFT(CoT) data1** 作为冷启动数据。
3. **冷启动 + RL 训练**：`DeepSeek V3` 使用 **data1** 进行冷启动，再进行 纯RL训练，得到`R1-checkp1` 然后经过一致性 RL 训练生成 `R1-checkp2` 。
    - 一致性指的是语言的一致性，`R1-checkp2` 解决了语言混杂问题。
    - `R1-checkp2`  生成更高质量的 **SFT(CoT) data2**
4. **基模型优化**：`DeepSeek V3` 通过 **SFT(CoT) data2** 训练生成 新的 `DeepSeek V3`
    - 生成 通用知识 **SFT(Knowledge) data3**
5. **最终模型**：`DeepSeek V3` 使用 **data 2** + **data3** 进行冷启动，结合 RL 和人类偏好训练，生成 `DeepSeek R1`。

### GPRO

## R1 的破圈：**行业影响与未来展望**

**R1 突破的基础是真正的实力。**性能确实非常强大，懂的都懂。当然，DeepSeek 做到的还不止这些，他还做了很多尝试，也分享了全球的AI行业

R1 的成功不仅在于其强大的性能，更在于它解决了行业面临的难题，并为学术界指明了发展方向。如果你是研究人员或者大模型公司，你会觉得这东西想我所想，急我所急。

DeepSeek 的贡献不仅限于 R1，还包括以下几点：

1. **蒸馏实验**：通过蒸馏实验证明了高质量 reasoning CoT 能够激发现有模型的能力。这一发现为现有模型的优化提供了新的思路。
2. **创新范式**：R1 证明了「左脚踩右脚」的可行性，为未来几个月到半年内的 RL 领域突破奠定了基础。
3. **AI Infra 的崛起**：全球 AI 基础设施终于找到了值得部署的模型，推动了 AI Infra 的发展。如果 OpenAI 开源，全球 AI Infra 的格局将大不相同。
4. **可控推理时间扩展**：虽然 R1 尝试了 RL，但尚未进行可控的 inference time scaling。这一方向的探索有望进一步提升模型性能。
5. **long2short 推理**：这一概念非常有趣，推理过程不应被视为模型的旁路思考，而是 Next Token Prediction 的一部分。未来，推理过程将更加高效，能够快速聚焦于关键问题。

新的尝试

- **基模型的重要性**：即使是通过“左脚踩右脚”的方式提升模型，基模型的质量仍然至关重要。

# Relative Article

[**万字赏析 DeepSeek 创造之美：DeepSeek R1 是怎样炼成的？**](https://mp.weixin.qq.com/s/8ifsQ1eRJpOSHCrFAtAX0A)

[万字赏析 DeepSeek 创造之美：DeepSeek R1 是怎样炼成的？丨荐读.pdf](/assets/pdf/DeepSeekR1_Analysis_Tenction.pdf)

[https://www.youtube.com/watch?v=axlQI7fGn_8&t=0s](https://www.youtube.com/watch?v=axlQI7fGn_8&t=0s)
